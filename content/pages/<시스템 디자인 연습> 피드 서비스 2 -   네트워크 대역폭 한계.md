---
title: <시스템 디자인 연습> 피드 서비스 2 - 네트워크 대역폭 한계
categories:
  - 백엔드
tags:
  - 부하테스트
  - django
  - aws
date: 2023-11-15T15:21:15+09:00
draft: false
modified: 2023-12-29T11:02:17+09:00
featured: true
series: 시스템 디자인 연습
---
### 작업
두번째 실험에서는 AWS ECS Fargate 를 이용해 페이지 조회 작업을 스케일 아웃했다. 첫번째 실험과 같은 조건인데, 100개의 짧은 글(post)이 로드된 페이지를 읽는 작업이다.

### 목표
1000 RPS를 견디는 환경을 만든다. 이를 위해 가상 사용자(VU) 1000명을 설정하고, 1초 간격으로 3분 동안 요청을 지속했다.

### 결과 요약
1. 물리적 네트워크 대역폭이 병목인 상황을 확인하고 테스트 환경까지 클라우드에 구축했다.
2. 위 목표를 달성하기 위해 Fargate 기준 vCPU 8개(1024 * 8)가 필요하다는 것을 확인했다.

### 환경
> AWS  
서버: Django + gunicorn  
DB: SQLite or MySQL or Redis  
인프라: AWS ECS Fargate, Docker  
테스팅: K6 + Grafana + InfluxDB  

### 결과 상세
AWS Fargate CPU와 메모리를 증가시키며 실험했지만, RPS가 200 수준에서 더 이상 개선되지 않았다. CPU와 메모리 사용량에 여유가 있음에도 불구하고 성능이 개선되지 않아 다른 병목 요소가 있을 것으로 추측하고 원인을 찾기 시작했다.

{{< image src="/images/282401978-528d44e5-dbf8-4d03-93ee-3dc89633f05e.png" >}}

	[AWS fargate 설정]
	태스크: 2개
	태스크 CPU: 4 vCPU
	태스크 메모리: 8 GB

> 결과  
> checks.........................: 99.73%   
> data_received..................: 2.1 GB 10 MB/s  
> http_reqs......................: 42910  208.976842/s  
> iteration_duration.............: avg=4.24s    p(95)=10.47s  


사용 중이던 DB(SQLite)를 AWS RDS(MySQL)와 ElastiCache(Redis)로 각각 변경했지만, 결과는 동일했다. 설정한 RDS와 ElastiCache의 지표도 정상이어서 DB 병목이 아님을 확인했다.

{{< image src="/images/282452209-4b5f5034-679f-4ec4-9def-cdec7d3d37a1.png" >}}

	[AWS RDS(mysql) 설정]
	인스턴스 클래스: db.t3.micro
	vCPU: 2
	RAM: 1GB
	네트워크: 2085Mbps(256MBps)
	
	[AWS ElastiCache(Redis) 설정]
	노드 유형: cache.t3.micro
	memory: 0.5GiB
	network: Up to 5Gbps(625MBps)

> 결과  
> checks.........................: 99.80%   
> data_received..................: 2.1 GB 10 MB/s  
> http_reqs......................: 43381  208.450836/s  
> iteration_duration.............: avg=4.19s   p(95)=10.17s  

지표를 확인하던 중 K6의 데이터 수신 속도(data_received)가 항상 초당 10 MB에 머무는 것을 발견했다. 이는 로컬 환경에서 기록한 17 MB/s 보다 낮았다. 그제서야 테스트를 수행하는 클라이언트 역시 실험의 변수가 될 수 있음을 깨달았다. 집에서 사용하는 100mbps 인터넷의 실제 속도가 90mbps 가량이었고, 이는 초당 데이터 송수신량이 최대 11.25MBps로 제한됨을 의미했다.

AWS EC2 인스턴스에 테스팅 환경을 새로 구축한 후, data_received가 17 MB/s로 증가했고, CPU 사용량이 99%에 도달하는 것을 확인할 수 있었다.

{{< image src="/images/282753788-e4eeab4e-8222-40ff-8e2d-a5eb1f199fcf.png" >}}

	[Client(AWS EC2) 설정]
	인스턴스 유형: c6gn.large
	vCPU 수: 2
	Memory: 4 GiB
	Network Bandwidth: Up to 25Gbps
	
	[AWS fargate 설정]
	태스크: 1개
	태스크 CPU: 4 vCPU
	태스크 메모리: 8 GiB

> 결과  
> checks.........................: 99.78%  
> data_received..................: 3.2 GB 17 MB/s  
> http_reqs......................: 65565  358.497221/s  
> iterations.....................: 65565  358.497221/s


최종적으로 Fargate 태스크 수를 늘려 아래와 같은 결과를 얻었다. 목표로 했던 RPS 1000에 거의 도달할 수 있었다.

{{< image src="/images/282758981-79a8c1c5-22f4-4543-897e-3380a7e4ed0e.png" >}}

	[AWS fargate 설정]
	태스크: 2개
	태스크 CPU: 4 vCPU
	태스크 메모리: 8 GiB

> 결과  
> checks.........................: 99.96%  
> data_received..................: 7.2 GB 40 MB/s  
> http_reqs......................: 147692 813.172902/s  
> iteration_duration.............: avg=1.22s    p(95)=1.93s

## refs
- [sample-feed 3차 · Issue #3 · chankoo/load-testing-practices · GitHub](https://github.com/chankoo/load-testing-practices/issues/3)


## links
- 
